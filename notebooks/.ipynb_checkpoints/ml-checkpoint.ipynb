{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8da932-9e84-42ea-a73c-bac4aa9ac4bf",
   "metadata": {},
   "source": [
    "Here to allow fast results reproducability we train a model on a small sample of dataset (it can be disabled setting use_sample flag to False). File ml.py copy the following content for automated pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d76f0d-38df-4442-9bd2-04870cf8a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, sin, cos, lit, udf, isnan, count, isnull, expr, rand\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, BooleanType\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import time\n",
    "\n",
    "def run(command):\n",
    "    \"\"\"Function for executing terminal commands\"\"\"\n",
    "    return os.popen(command).read()\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 21\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"/user/team21/project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a985ce-b1bd-4fff-9199-10f44c0064c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeatureTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Transformer to convert time and date features into cyclical features\n",
    "    Handles NULL values and ensures robustness\n",
    "    \"\"\"\n",
    "    def _transform(self, dataset):\n",
    "        def parse_time(time_val):\n",
    "            \"\"\"Converts time from HHMM format to hours and hour fractions\"\"\"\n",
    "            if time_val is None or time_val < 0:\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                time_str = str(int(time_val))\n",
    "                if len(time_str) <= 2:  # Only minutes\n",
    "                    return float(time_str) / 60.0\n",
    "\n",
    "                # Hours and minutes\n",
    "                if len(time_str) == 3:\n",
    "                    time_str = \"0\" + time_str\n",
    "                hours = int(time_str[0:2])\n",
    "                minutes = int(time_str[2:4])\n",
    "\n",
    "                return float(hours) + float(minutes) / 60.0\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        # Function to extract month from date in yyyymmdd format\n",
    "        def get_month_from_date(date_val):\n",
    "            if date_val is None or date_val == \"\":\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                date_str = str(date_val).strip().split('-')\n",
    "                return int(date_str[1])  # month\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        # determine day of week (1=Monday, 7=Sunday)\n",
    "        def get_day_of_week(date_val):\n",
    "            \"\"\"Calculates day of week from date in yyyymmdd format\"\"\"\n",
    "            if date_val is None or date_val == \"\":\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                date_str = str(date_val).strip().split('-')\n",
    "                year = int(int(date_str[0]))\n",
    "                month = int(date_str[1])\n",
    "                day = int(date_str[2])\n",
    "                if year < 1900 or year > 2100 or month < 1 or month > 12 or day < 1 or day > 31:\n",
    "                    return None\n",
    "                if month < 3:\n",
    "                    month += 12\n",
    "                    year -= 1\n",
    "\n",
    "                k = year % 100\n",
    "                j = year // 100\n",
    "\n",
    "                h = (day + ((13 * (month + 1)) // 5) + k + (k // 4) + (j // 4) - (2 * j)) % 7\n",
    "                day_of_week = ((h + 5) % 7) + 1\n",
    "\n",
    "                return day_of_week\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        parse_time_udf = udf(parse_time, DoubleType())\n",
    "        get_month_udf = udf(get_month_from_date, IntegerType())\n",
    "        get_day_of_week_udf = udf(get_day_of_week, IntegerType())\n",
    "        dataset = dataset.withColumn(\"dep_time_decimal\",\n",
    "                              when(col(\"CRS_DEP_TIME\").isNotNull(),\n",
    "                                   parse_time_udf(col(\"CRS_DEP_TIME\"))).otherwise(0.0))\n",
    "        dataset = dataset.withColumn(\"arr_time_decimal\",\n",
    "                              when(col(\"CRS_ARR_TIME\").isNotNull(),\n",
    "                                   parse_time_udf(col(\"CRS_ARR_TIME\"))).otherwise(0.0))\n",
    "        dataset = dataset.withColumn(\"dep_time_sin\",\n",
    "                              sin(col(\"dep_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"dep_time_cos\",\n",
    "                              cos(col(\"dep_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"arr_time_sin\",\n",
    "                              sin(col(\"arr_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"arr_time_cos\",\n",
    "                              cos(col(\"arr_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"month\",\n",
    "                          when(col(\"fl_date\").isNotNull(),\n",
    "                                get_month_udf(col(\"fl_date\"))).otherwise(6))\n",
    "        dataset = dataset.withColumn(\"dayofweek\",\n",
    "                              when(col(\"fl_date\").isNotNull(),\n",
    "                                    get_day_of_week_udf(col(\"fl_date\"))).otherwise(4))\n",
    "\n",
    "        dataset = dataset.withColumn(\"month_sin\", sin(col(\"month\") * 2 * math.pi / 12))\n",
    "        dataset = dataset.withColumn(\"month_cos\", cos(col(\"month\") * 2 * math.pi / 12))\n",
    "        dataset = dataset.withColumn(\"dayofweek_sin\", sin(col(\"dayofweek\") * 2 * math.pi / 7))\n",
    "        dataset = dataset.withColumn(\"dayofweek_cos\", cos(col(\"dayofweek\") * 2 * math.pi / 7))\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbd49ae-e03c-4221-a798-b8bed7a930fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(full_table_name):\n",
    "    flights_df = spark.read.table(full_table_name)\n",
    "    flights_df = flights_df.withColumn(\"fl_date\", col(\"fl_date\").cast(\"string\"))\n",
    "    flights_df = flights_df.withColumn(\n",
    "        \"is_delayed\",\n",
    "        when((col(\"ARR_DELAY\").isNotNull()) & (col(\"ARR_DELAY\") > 0), 1).otherwise(0)\n",
    "    )\n",
    "    flights_df = flights_df.withColumn(\"label\", col(\"is_delayed\").cast(DoubleType()))\n",
    "    available_cols = [col_name.lower() for col_name in\n",
    "                     [\"FL_DATE\", \"AIRLINE_CODE\", \"FL_NUMBER\", \"ORIGIN\", \"ORIGIN_CITY\",\n",
    "                      \"DEST\", \"DEST_CITY\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\",\n",
    "                      \"CRS_ELAPSED_TIME\", \"DISTANCE\"]\n",
    "                     if col_name.lower() in flights_df.columns]\n",
    "    flights_df = flights_df.select([\"label\"] + available_cols)\n",
    "    for col_name in available_cols:\n",
    "        flights_df = flights_df.filter(col(col_name).isNotNull())\n",
    "\n",
    "    return flights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc4940e-d0d7-4074-8928-a1de2190710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1.0% sample of data for model development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data: 7906 delayed flights, 16398 on-time flights\n",
      "Class imbalance ratio: 2.07\n",
      "Undersampling on-time flights with fraction: 0.4821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training data: 7906 delayed flights, 7952 on-time flights\n",
      "New class ratio: 1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training dataset size: 15858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size (original distribution): 5976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced train and test datasets to HDFS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, fl_date='2023-07-12', airline_code='DL', fl_number=2326, origin='ATL', origin_city='Atlanta, GA', dest='JAX', dest_city='Jacksonville, FL', crs_dep_time=855, crs_arr_time=1008, crs_elapsed_time=73.0, distance=270.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"team21_projectdb_v4.flights_optimized\"\n",
    "flights_df = prepare_data(data_path)\n",
    "\n",
    "# Optional: Take a smaller sample for development\n",
    "sample_fraction = 0.01\n",
    "use_sample = True\n",
    "\n",
    "if use_sample:\n",
    "    print(f\"Using {sample_fraction*100}% sample of data for model development\")\n",
    "    flights_df = flights_df.sample(fraction=sample_fraction, seed=42)\n",
    "\n",
    "train_data, test_data = flights_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "positive_count = train_data.filter(train_data.label == 1.0).count()\n",
    "negative_count = train_data.filter(train_data.label == 0.0).count()\n",
    "\n",
    "print(f\"Original training data: {positive_count} delayed flights, {negative_count} on-time flights\")\n",
    "print(f\"Class imbalance ratio: {negative_count / positive_count:.2f}\")\n",
    "\n",
    "negative_data = train_data.filter(train_data.label == 0.0)\n",
    "positive_data = train_data.filter(train_data.label == 1.0)\n",
    "sampling_fraction = float(positive_count) / float(negative_count)\n",
    "\n",
    "print(f\"Undersampling on-time flights with fraction: {sampling_fraction:.4f}\")\n",
    "\n",
    "undersampled_negative = negative_data.sample(fraction=sampling_fraction, seed=42)\n",
    "balanced_train_data = positive_data.union(undersampled_negative)\n",
    "shuffled_df = balanced_train_data.orderBy(rand())\n",
    "\n",
    "balanced_pos_count = balanced_train_data.filter(balanced_train_data.label == 1.0).count()\n",
    "balanced_neg_count = balanced_train_data.filter(balanced_train_data.label == 0.0).count()\n",
    "\n",
    "print(f\"Balanced training data: {balanced_pos_count} delayed flights, {balanced_neg_count} on-time flights\")\n",
    "print(f\"New class ratio: {balanced_neg_count / balanced_pos_count:.2f}\")\n",
    "\n",
    "balanced_train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"Balanced training dataset size: {balanced_train_data.count()}\")\n",
    "print(f\"Test dataset size (original distribution): {test_data.count()}\")\n",
    "\n",
    "balanced_train_data.write.mode(\"overwrite\").format(\"json\").save(\"/user/team21/project/data/train\")\n",
    "test_data.write.mode(\"overwrite\").format(\"json\").save(\"/user/team21/project/data/test\")\n",
    "print(\"Saved balanced train and test datasets to HDFS\")\n",
    "\n",
    "train_data = balanced_train_data\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4459579a-4d0b-4044-99c0-e0011d9636f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>(109 + 1) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions after repartition: 4\n"
     ]
    }
   ],
   "source": [
    "flights_df = flights_df.repartition(4)\n",
    "print(\"Partitions after repartition:\", flights_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cae6f2-6258-4c40-9303-24db79b98c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using categorical columns: ['airline_code', 'origin', 'dest']\n",
      "Using numeric columns: ['fl_number', 'crs_elapsed_time', 'distance']\n",
      "Applying feature preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------+-----------------+-----------------+------------------------+----------------+-------------------+--------------------+\n",
      "|label|airline_code_encoded| origin_encoded|     dest_encoded|fl_number_imputed|crs_elapsed_time_imputed|distance_imputed|       dep_time_sin|        dep_time_cos|\n",
      "+-----+--------------------+---------------+-----------------+-----------------+------------------------+----------------+-------------------+--------------------+\n",
      "|  1.0|      (19,[2],[1.0])|(329,[1],[1.0])| (327,[46],[1.0])|             2474|                   147.0|          1017.0|-0.7933533402912357|  0.6087614290087199|\n",
      "|  1.0|     (19,[13],[1.0])|(329,[1],[1.0])| (327,[93],[1.0])|             5970|                    75.0|           309.0|-0.9940563382223196| 0.10886687485196471|\n",
      "|  1.0|      (19,[8],[1.0])|(329,[0],[1.0])|(327,[301],[1.0])|             3357|                    88.0|           433.0|-0.8724960070727971| 0.48862124149695496|\n",
      "|  1.0|      (19,[4],[1.0])|(329,[3],[1.0])|  (327,[2],[1.0])|              254|                   135.0|           888.0|               -1.0|-1.83697019872102...|\n",
      "|  1.0|      (19,[6],[1.0])|(329,[2],[1.0])|(327,[171],[1.0])|             3696|                    57.0|           135.0|-0.8064446042674829|   0.591309648363582|\n",
      "+-----+--------------------+---------------+-----------------+-----------------+------------------------+----------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Feature columns for vector assembler: ['airline_code_encoded', 'origin_encoded', 'dest_encoded', 'fl_number_imputed', 'crs_elapsed_time_imputed', 'distance_imputed', 'dep_time_sin', 'dep_time_cos', 'arr_time_sin', 'arr_time_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 22:06:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final feature vectors with weights:\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(686,[2,20,394,67...|\n",
      "|  1.0|(686,[13,20,441,6...|\n",
      "|  1.0|(686,[8,19,649,67...|\n",
      "|  1.0|(686,[4,22,350,67...|\n",
      "|  1.0|(686,[6,21,519,67...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [col.lower() for col in [\"AIRLINE_CODE\", \"ORIGIN\", \"DEST\"]]\n",
    "print(f\"Using categorical columns: {categorical_cols}\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", handleInvalid=\"keep\")\n",
    "           for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_indexed\", outputCol=f\"{c}_encoded\", handleInvalid=\"keep\")\n",
    "           for c in categorical_cols]\n",
    "\n",
    "time_transformer = TimeFeatureTransformer()\n",
    "numeric_cols = [col.lower() for col in [\"FL_NUMBER\", \"CRS_ELAPSED_TIME\", \"DISTANCE\"]]\n",
    "\n",
    "print(f\"Using numeric columns: {numeric_cols}\")\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{col}_imputed\" for col in numeric_cols],\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "numeric_cols_imputed = [f\"{col}_imputed\" for col in numeric_cols]\n",
    "preprocessing_stages = indexers + encoders + [time_transformer, imputer]\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "print(\"Applying feature preprocessing...\")\n",
    "preprocessing_model = preprocessing_pipeline.fit(train_data)\n",
    "train_preprocessed = preprocessing_model.transform(train_data)\n",
    "test_preprocessed = preprocessing_model.transform(test_data)\n",
    "\n",
    "print(\"Preprocessed data sample:\")\n",
    "train_preprocessed.select(\"label\", *([f\"{c}_encoded\" for c in categorical_cols] +\n",
    "                                    numeric_cols_imputed +\n",
    "                                    [\"dep_time_sin\", \"dep_time_cos\"])).show(5)\n",
    "\n",
    "for column in numeric_cols_imputed:\n",
    "    train_preprocessed = train_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "    test_preprocessed = test_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "\n",
    "for column in [\"dep_time_sin\", \"dep_time_cos\", \"arr_time_sin\", \"arr_time_cos\"]:\n",
    "    train_preprocessed = train_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "    test_preprocessed = test_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "\n",
    "if \"fl_date\" in train_data.columns:\n",
    "    for column in [\"month_sin\", \"month_cos\", \"dayofweek_sin\", \"dayofweek_cos\"]:\n",
    "        if column in train_preprocessed.columns:\n",
    "            train_preprocessed = train_preprocessed.withColumn(\n",
    "                column,\n",
    "                when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "            )\n",
    "            test_preprocessed = test_preprocessed.withColumn(\n",
    "                column,\n",
    "                when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "            )\n",
    "\n",
    "feature_cols = []\n",
    "if categorical_cols:\n",
    "    feature_cols.extend([f\"{c}_encoded\" for c in categorical_cols])\n",
    "feature_cols.extend(numeric_cols_imputed)\n",
    "feature_cols.extend([\"dep_time_sin\", \"dep_time_cos\", \"arr_time_sin\", \"arr_time_cos\"])\n",
    "if \"fl_date\" in train_data.columns:\n",
    "    feature_cols.extend([\"month_sin\", \"month_cos\", \"dayofweek_sin\", \"dayofweek_cos\"])\n",
    "\n",
    "print(f\"\\nFeature columns for vector assembler: {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withMean=False,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[assembler, scaler])\n",
    "feature_model = feature_pipeline.fit(train_preprocessed)\n",
    "train_vectorized = feature_model.transform(train_preprocessed)\n",
    "test_vectorized = feature_model.transform(test_preprocessed)\n",
    "\n",
    "train_vectorized = train_vectorized.withColumn(\"features\", col(\"features_scaled\"))\n",
    "test_vectorized = test_vectorized.withColumn(\"features\", col(\"features_scaled\"))\n",
    "\n",
    "train_vectorized = train_vectorized.na.drop(subset=[\"features\"])\n",
    "test_vectorized = test_vectorized.na.drop(subset=[\"features\"])\n",
    "\n",
    "\n",
    "print(\"\\nFinal feature vectors with weights:\")\n",
    "train_vectorized.select(\"label\", \"features\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c87447-ce41-4601-955a-1b6e42b9a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 22:08:15 WARN DAGScheduler: Broadcasting large task binary with size 1113.4 KiB\n",
      "25/05/09 22:08:25 WARN DAGScheduler: Broadcasting large task binary with size 1286.1 KiB\n",
      "25/05/09 22:10:14 WARN DAGScheduler: Broadcasting large task binary with size 1113.4 KiB\n",
      "25/05/09 22:10:22 WARN DAGScheduler: Broadcasting large task binary with size 1286.2 KiB\n",
      "25/05/09 22:10:32 WARN DAGScheduler: Broadcasting large task binary with size 1484.8 KiB\n",
      "25/05/09 22:10:42 WARN DAGScheduler: Broadcasting large task binary with size 1706.1 KiB\n",
      "25/05/09 22:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1949.8 KiB\n",
      "25/05/09 22:11:05 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/05/09 22:11:17 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/09 22:11:30 WARN DAGScheduler: Broadcasting large task binary with size 1625.8 KiB\n",
      "25/05/09 22:12:20 WARN DAGScheduler: Broadcasting large task binary with size 1003.3 KiB\n",
      "25/05/09 22:12:29 WARN DAGScheduler: Broadcasting large task binary with size 1211.9 KiB\n",
      "25/05/09 22:12:39 WARN DAGScheduler: Broadcasting large task binary with size 1472.9 KiB\n",
      "25/05/09 22:12:49 WARN DAGScheduler: Broadcasting large task binary with size 1780.5 KiB\n",
      "25/05/09 22:13:01 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/09 22:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1543.0 KiB\n",
      "25/05/09 22:14:02 WARN DAGScheduler: Broadcasting large task binary with size 1003.3 KiB\n",
      "25/05/09 22:14:11 WARN DAGScheduler: Broadcasting large task binary with size 1211.9 KiB\n",
      "25/05/09 22:14:21 WARN DAGScheduler: Broadcasting large task binary with size 1472.9 KiB\n",
      "25/05/09 22:14:32 WARN DAGScheduler: Broadcasting large task binary with size 1780.5 KiB\n",
      "25/05/09 22:14:45 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/09 22:14:58 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/09 22:15:12 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/05/09 22:15:28 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/05/09 22:15:44 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/05/09 22:16:04 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/05/09 22:16:25 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/09 22:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1137.4 KiB\n",
      "25/05/09 22:18:04 WARN DAGScheduler: Broadcasting large task binary with size 1318.2 KiB\n",
      "25/05/09 22:18:31 WARN DAGScheduler: Broadcasting large task binary with size 1009.4 KiB\n",
      "25/05/09 22:19:29 WARN DAGScheduler: Broadcasting large task binary with size 1137.4 KiB\n",
      "25/05/09 22:19:37 WARN DAGScheduler: Broadcasting large task binary with size 1318.2 KiB\n",
      "25/05/09 22:19:46 WARN DAGScheduler: Broadcasting large task binary with size 1520.6 KiB\n",
      "25/05/09 22:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1744.2 KiB\n",
      "25/05/09 22:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1994.0 KiB\n",
      "25/05/09 22:20:17 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/05/09 22:20:28 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/09 22:20:41 WARN DAGScheduler: Broadcasting large task binary with size 1665.2 KiB\n",
      "25/05/09 22:21:33 WARN DAGScheduler: Broadcasting large task binary with size 1201.8 KiB\n",
      "25/05/09 22:21:42 WARN DAGScheduler: Broadcasting large task binary with size 1462.0 KiB\n",
      "25/05/09 22:21:52 WARN DAGScheduler: Broadcasting large task binary with size 1772.0 KiB\n",
      "25/05/09 22:22:04 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/09 22:22:17 WARN DAGScheduler: Broadcasting large task binary with size 1525.1 KiB\n",
      "25/05/09 22:23:16 WARN DAGScheduler: Broadcasting large task binary with size 1201.8 KiB\n",
      "25/05/09 22:23:26 WARN DAGScheduler: Broadcasting large task binary with size 1462.0 KiB\n",
      "25/05/09 22:23:38 WARN DAGScheduler: Broadcasting large task binary with size 1772.0 KiB\n",
      "25/05/09 22:23:50 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/09 22:24:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/09 22:24:18 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/05/09 22:24:33 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/05/09 22:24:51 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/05/09 22:25:10 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/05/09 22:25:30 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/09 22:26:55 WARN DAGScheduler: Broadcasting large task binary with size 1151.6 KiB\n",
      "25/05/09 22:27:03 WARN DAGScheduler: Broadcasting large task binary with size 1338.2 KiB\n",
      "25/05/09 22:27:29 WARN DAGScheduler: Broadcasting large task binary with size 1013.2 KiB\n",
      "25/05/09 22:28:30 WARN DAGScheduler: Broadcasting large task binary with size 1151.6 KiB\n",
      "25/05/09 22:28:39 WARN DAGScheduler: Broadcasting large task binary with size 1338.2 KiB\n",
      "25/05/09 22:28:48 WARN DAGScheduler: Broadcasting large task binary with size 1546.7 KiB\n",
      "25/05/09 22:28:58 WARN DAGScheduler: Broadcasting large task binary with size 1784.6 KiB\n",
      "25/05/09 22:29:09 WARN DAGScheduler: Broadcasting large task binary with size 2047.7 KiB\n",
      "25/05/09 22:29:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/05/09 22:29:33 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/05/09 22:29:46 WARN DAGScheduler: Broadcasting large task binary with size 1696.8 KiB\n",
      "25/05/09 22:30:38 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "25/05/09 22:30:48 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "25/05/09 22:30:59 WARN DAGScheduler: Broadcasting large task binary with size 1762.7 KiB\n",
      "25/05/09 22:31:11 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/09 22:31:25 WARN DAGScheduler: Broadcasting large task binary with size 1528.4 KiB\n",
      "25/05/09 22:32:15 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "25/05/09 22:32:24 WARN DAGScheduler: Broadcasting large task binary with size 1452.2 KiB\n",
      "25/05/09 22:32:34 WARN DAGScheduler: Broadcasting large task binary with size 1762.7 KiB\n",
      "25/05/09 22:32:45 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/09 22:32:58 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/09 22:33:11 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/05/09 22:33:25 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/05/09 22:33:41 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "25/05/09 22:33:59 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/05/09 22:34:18 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/09 22:36:02 WARN DAGScheduler: Broadcasting large task binary with size 1215.7 KiB\n",
      "25/05/09 22:36:12 WARN DAGScheduler: Broadcasting large task binary with size 1523.0 KiB\n",
      "25/05/09 22:36:23 WARN DAGScheduler: Broadcasting large task binary with size 1885.1 KiB\n",
      "25/05/09 22:36:35 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/05/09 22:36:49 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/05/09 22:37:04 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/05/09 22:37:22 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/05/09 22:37:40 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/05/09 22:38:01 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/05/09 22:38:25 WARN TaskSetManager: Stage 607 contains a task of very large size (1443 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/09 22:38:26 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/09 22:38:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/09 22:38:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/09 22:38:58 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "[Stage 625:====================================================>(108 + 1) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest - AUC: 0.6085235796282373, Precision: 0.6338412338155843, Recall: 0.5831659973226239, F1: 0.5966683851074968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "rf_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "rf_cv = CrossValidator(estimator=rf,\n",
    "                      estimatorParamMaps=rf_param_grid,\n",
    "                      evaluator=rf_evaluator,\n",
    "                      numFolds=3)\n",
    "\n",
    "rf_model = rf_cv.fit(train_vectorized)\n",
    "best_rf_model = rf_model.bestModel\n",
    "\n",
    "best_rf_model.write().overwrite().save(\"project/models/model1\")\n",
    "rf_predictions = best_rf_model.transform(test_vectorized)\n",
    "\n",
    "rf_roc = rf_evaluator.evaluate(rf_predictions)\n",
    "rf_evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "rf_precision = rf_evaluator_precision.evaluate(rf_predictions)\n",
    "rf_evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "rf_recall = rf_evaluator_recall.evaluate(rf_predictions)\n",
    "rf_evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "rf_f1 = rf_evaluator_f1.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random forest - AUC: {rf_roc}, Precision: {rf_precision}, Recall: {rf_recall}, F1: {rf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73bc9eb-2ea5-4fbb-8e01-5a5919d5052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best maxDepth: 15\n",
      "Best numTrees: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best maxDepth: {best_rf_model.getMaxDepth()}\")\n",
    "print(f\"Best numTrees: {best_rf_model.getNumTrees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5d3846-1f87-4f03-a591-e7c537615693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get: `/home/team21/flight-delay-prediction/models/model1/data/_SUCCESS': File exists\n",
      "get: `/home/team21/flight-delay-prediction/models/model1/metadata/_SUCCESS': File exists\n",
      "get: `/home/team21/flight-delay-prediction/models/model1/metadata/part-00000': File exists\n",
      "get: `/home/team21/flight-delay-prediction/models/model1/treesMetadata/_SUCCESS': File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"hdfs dfs -get project/models/model1 \\\n",
    "~/flight-delay-prediction/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "555f7f42-aad2-409a-a12b-f6cf93c4c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a468ecad-e447-4409-be4f-ee6a2a442072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 22:40:12 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "get: `/home/team21/project/big-data-pipeline-project/models/.': No such file or directory: `file:///home/team21/project/big-data-pipeline-project/models'\n",
      "25/05/09 22:40:29 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/09 22:40:39 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/09 22:40:49 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/09 22:40:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 637:===================================================> (107 + 1) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Metrics:\n",
      "  True Positives: 1107\n",
      "  False Positives: 1680\n",
      "  True Negatives: 2378\n",
      "  False Negatives: 811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_predictions.select(\"label\", \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .csv(\"/user/team21/project/output/model1_predictions\", header=True, mode=\"overwrite\")\n",
    "\n",
    "run(\"hdfs dfs -mv /user/team21/project/output/model1_predictions/*.csv /user/team21/project/output/model1_predictions/model1_predictions.csv\")\n",
    "run(\"hdfs dfs -cat /user/team21/project/output/model1_predictions/model1_predictions.csv > ~/flight-delay-prediction/output/model1_predictions.csv\")\n",
    "run(\"hdfs dfs -get /user/team21/project/models/model1 ~/project/big-data-pipeline-project/models/.\")\n",
    "\n",
    "# After making predictions, calculate per-class metrics\n",
    "rf_tp = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_fp = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_tn = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "rf_fn = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nDetailed Classification Metrics:\")\n",
    "print(f\"  True Positives: {rf_tp}\")\n",
    "print(f\"  False Positives: {rf_fp}\")\n",
    "print(f\"  True Negatives: {rf_tn}\")\n",
    "print(f\"  False Negatives: {rf_fn}\")\n",
    "\n",
    "# For potential threshold adjustment\n",
    "if rf_tn == 0 and rf_fp > 0:\n",
    "    print(\"\\nWARNING: Model is predicting all examples as positive.\")\n",
    "    print(\"Consider adjusting threshold or balancing training data differently.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd155bab-9a49-4615-9078-e05ea171ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probability(v, index):\n",
    "    try:\n",
    "        return float(v[index])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "prob_udf = udf(lambda v: extract_probability(v, 1), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0f3a0a-bedc-49f0-ae8e-fbac4f91799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree model with hyperparameter tuning...\n",
      "Performing cross-validation with grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree parameters: {Param(parent='DecisionTreeClassifier_e0cf332bab20', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='featuresCol', doc='features column name.'): 'features', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='labelCol', doc='label column name.'): 'label', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 64, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='DecisionTreeClassifier_e0cf332bab20', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='DecisionTreeClassifier_e0cf332bab20', name='seed', doc='random seed.'): 463344090795272856}\n",
      "Best maxDepth: 10\n",
      "Best maxBins: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1112:===================================================>(109 + 1) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Model Evaluation:\n",
      "  AUC: 0.5180\n",
      "  Precision: 0.6081\n",
      "  Recall: 0.5395\n",
      "  F1 Score: 0.5546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "print(\"Training Decision Tree model with hyperparameter tuning...\")\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [7, 10]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "dt_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "dt_cv = CrossValidator(\n",
    "    estimator=decision_tree,\n",
    "    estimatorParamMaps=dt_param_grid,\n",
    "    evaluator=dt_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "print(\"Performing cross-validation with grid search...\")\n",
    "dt_model = dt_cv.fit(train_vectorized)\n",
    "\n",
    "best_dt_model = dt_model.bestModel\n",
    "print(f\"Best Decision Tree parameters: {best_dt_model.extractParamMap()}\")\n",
    "print(f\"Best maxDepth: {best_dt_model.getMaxDepth()}\")\n",
    "print(f\"Best maxBins: {best_dt_model.getMaxBins()}\")\n",
    "\n",
    "best_dt_model.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "dt_predictions = best_dt_model.transform(test_vectorized)\n",
    "dt_predictions = dt_predictions.withColumn(\n",
    "    \"probability_for_delay\",\n",
    "    prob_udf(col(\"probability\"))\n",
    ")\n",
    "\n",
    "dt_roc = dt_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "dt_evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "dt_precision = dt_evaluator_precision.evaluate(dt_predictions)\n",
    "\n",
    "dt_evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "dt_recall = dt_evaluator_recall.evaluate(dt_predictions)\n",
    "\n",
    "dt_evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "dt_f1 = dt_evaluator_f1.evaluate(dt_predictions)\n",
    "\n",
    "print(\"\\nDecision Tree Model Evaluation:\")\n",
    "print(f\"  AUC: {dt_roc:.4f}\")\n",
    "print(f\"  Precision: {dt_precision:.4f}\")\n",
    "print(f\"  Recall: {dt_recall:.4f}\")\n",
    "print(f\"  F1 Score: {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b868c3f-ee33-4a10-a339-e7ab3ac9e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c6eea0e-8455-4b8c-89e3-79ac009a8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 1111|\n",
      "|  0.0|       1.0| 1945|\n",
      "|  1.0|       0.0|  807|\n",
      "|  0.0|       0.0| 2113|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Metrics:\n",
      "  True Positives: 1111\n",
      "  False Positives: 1945\n",
      "  True Negatives: 2113\n",
      "  False Negatives: 807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: `/user/team21/project/output/model3_predictions/model2_predictions.csv': No such file or directory: `hdfs://hadoop-02.uni.innopolis.ru:8020/user/team21/project/output/model3_predictions/model2_predictions.csv'\n",
      "cat: `/user/team21/project/output/model2_predictions/model3_predictions.csv': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nConfusion Matrix:\")\n",
    "dt_predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "# Detailed metrics\n",
    "tp = dt_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "fp = dt_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "tn = dt_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "fn = dt_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "\n",
    "print(\"\\nDetailed Classification Metrics:\")\n",
    "print(f\"  True Positives: {tp}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  True Negatives: {tn}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "\n",
    "# Save predictions\n",
    "dt_predictions.select(\"label\", \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/user/team21/project/output/model2_predictions\", mode=\"overwrite\")\n",
    "\n",
    "# Move and rename output file\n",
    "run(\"hdfs dfs -mv /user/team21/project/output/model2_predictions/*.csv /user/team21/project/output/model3_predictions/model2_predictions.csv\")\n",
    "run(\"hdfs dfs -cat /user/team21/project/output/model2_predictions/model3_predictions.csv > ~/flight-delay-prediction/output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2345274d-c1d1-40b5-bb28-9f9c30823672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 22:52:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/09 22:53:10 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/09 22:53:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/09 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "          model       auc  precision    recall        f1  accuracy\n",
      "0  RandomForest  0.608524   0.633841  0.583166  0.596668  0.583166\n",
      "1  DecisionTree  0.518015   0.608061  0.539491  0.554620  0.539491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get: `models/': No such file or directory: `file:/home/team21/flight-delay-prediction/notebooks/models'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All models and evaluation results saved successfully!\n"
     ]
    }
   ],
   "source": [
    "rf_tp = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_fp = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_tn = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "rf_fn = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "\n",
    "dt_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "rf_accuracy = (rf_tp + rf_tn) / (rf_tp + rf_tn + rf_fp + rf_fn)\n",
    "\n",
    "comparison_data = {\n",
    "    'model': ['RandomForest', 'DecisionTree'],\n",
    "    'auc': [rf_roc, dt_roc],\n",
    "    'precision': [rf_precision, dt_precision],\n",
    "    'recall': [rf_recall, dt_recall],\n",
    "    'f1': [rf_f1, dt_f1],\n",
    "    'accuracy': [rf_accuracy, dt_accuracy],\n",
    "    'true_positives': [rf_tp, tp],\n",
    "    'false_positives': [rf_fp, fp],\n",
    "    'true_negatives': [rf_tn, tn],\n",
    "    'false_negatives': [rf_fn, fn]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df[['model', 'auc', 'precision', 'recall', 'f1', 'accuracy']])\n",
    "comparison_df.to_csv(\"~/flight-delay-prediction/output/evaluation.csv\", index=False)\n",
    "\n",
    "spark.createDataFrame(comparison_df) \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/user/team21/project/output/evaluation\", mode=\"overwrite\")\n",
    "\n",
    "run(\"hdfs dfs -get /user/team21/project/models/model2 models/\")\n",
    "\n",
    "print(\"\\nAll models and evaluation results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2036eb6c-8f21-4acc-9f60-7256a5afc8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"hdfs dfs -get project/models/model2 \\\n",
    "~/flight-delay-prediction/models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
