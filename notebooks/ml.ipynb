{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32bab24-66a4-4d5d-a2b8-7a608ab93036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 20:26:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/09 20:26:02 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/09 20:26:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>21 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7abc2bc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, sin, cos, lit, udf, isnan, count, isnull, expr, rand\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, BooleanType\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import time\n",
    "\n",
    "def run(command):\n",
    "    \"\"\"Function for executing terminal commands\"\"\"\n",
    "    return os.popen(command).read()\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 21\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"/user/team21/project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a985ce-b1bd-4fff-9199-10f44c0064c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeatureTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Transformer to convert time and date features into cyclical features\n",
    "    Handles NULL values and ensures robustness\n",
    "    \"\"\"\n",
    "    def _transform(self, dataset):\n",
    "        def parse_time(time_val):\n",
    "            \"\"\"Converts time from HHMM format to hours and hour fractions\"\"\"\n",
    "            if time_val is None or time_val < 0:\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                time_str = str(int(time_val))\n",
    "                if len(time_str) <= 2:  # Only minutes\n",
    "                    return float(time_str) / 60.0\n",
    "\n",
    "                # Hours and minutes\n",
    "                if len(time_str) == 3:\n",
    "                    time_str = \"0\" + time_str\n",
    "                hours = int(time_str[0:2])\n",
    "                minutes = int(time_str[2:4])\n",
    "\n",
    "                return float(hours) + float(minutes) / 60.0\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        # Function to extract month from date in yyyymmdd format\n",
    "        def get_month_from_date(date_val):\n",
    "            if date_val is None or date_val == \"\":\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                date_str = str(date_val).strip().split('-')\n",
    "                return int(date_str[1])  # month\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        # determine day of week (1=Monday, 7=Sunday)\n",
    "        def get_day_of_week(date_val):\n",
    "            \"\"\"Calculates day of week from date in yyyymmdd format\"\"\"\n",
    "            if date_val is None or date_val == \"\":\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                date_str = str(date_val).strip().split('-')\n",
    "                year = int(int(date_str[0]))\n",
    "                month = int(date_str[1])\n",
    "                day = int(date_str[2])\n",
    "                if year < 1900 or year > 2100 or month < 1 or month > 12 or day < 1 or day > 31:\n",
    "                    return None\n",
    "                if month < 3:\n",
    "                    month += 12\n",
    "                    year -= 1\n",
    "\n",
    "                k = year % 100\n",
    "                j = year // 100\n",
    "\n",
    "                h = (day + ((13 * (month + 1)) // 5) + k + (k // 4) + (j // 4) - (2 * j)) % 7\n",
    "                day_of_week = ((h + 5) % 7) + 1\n",
    "\n",
    "                return day_of_week\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        parse_time_udf = udf(parse_time, DoubleType())\n",
    "        get_month_udf = udf(get_month_from_date, IntegerType())\n",
    "        get_day_of_week_udf = udf(get_day_of_week, IntegerType())\n",
    "        dataset = dataset.withColumn(\"dep_time_decimal\",\n",
    "                              when(col(\"CRS_DEP_TIME\").isNotNull(),\n",
    "                                   parse_time_udf(col(\"CRS_DEP_TIME\"))).otherwise(0.0))\n",
    "        dataset = dataset.withColumn(\"arr_time_decimal\",\n",
    "                              when(col(\"CRS_ARR_TIME\").isNotNull(),\n",
    "                                   parse_time_udf(col(\"CRS_ARR_TIME\"))).otherwise(0.0))\n",
    "        dataset = dataset.withColumn(\"dep_time_sin\",\n",
    "                              sin(col(\"dep_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"dep_time_cos\",\n",
    "                              cos(col(\"dep_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"arr_time_sin\",\n",
    "                              sin(col(\"arr_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"arr_time_cos\",\n",
    "                              cos(col(\"arr_time_decimal\") * 2 * math.pi / 24))\n",
    "        dataset = dataset.withColumn(\"month\",\n",
    "                          when(col(\"fl_date\").isNotNull(),\n",
    "                                get_month_udf(col(\"fl_date\"))).otherwise(6))\n",
    "        dataset = dataset.withColumn(\"dayofweek\",\n",
    "                              when(col(\"fl_date\").isNotNull(),\n",
    "                                    get_day_of_week_udf(col(\"fl_date\"))).otherwise(4))\n",
    "\n",
    "        dataset = dataset.withColumn(\"month_sin\", sin(col(\"month\") * 2 * math.pi / 12))\n",
    "        dataset = dataset.withColumn(\"month_cos\", cos(col(\"month\") * 2 * math.pi / 12))\n",
    "        dataset = dataset.withColumn(\"dayofweek_sin\", sin(col(\"dayofweek\") * 2 * math.pi / 7))\n",
    "        dataset = dataset.withColumn(\"dayofweek_cos\", cos(col(\"dayofweek\") * 2 * math.pi / 7))\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbd49ae-e03c-4221-a798-b8bed7a930fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(full_table_name):\n",
    "    flights_df = spark.read.table(full_table_name)\n",
    "    flights_df = flights_df.withColumn(\"fl_date\", col(\"fl_date\").cast(\"string\"))\n",
    "    flights_df = flights_df.withColumn(\n",
    "        \"is_delayed\",\n",
    "        when((col(\"ARR_DELAY\").isNotNull()) & (col(\"ARR_DELAY\") > 0), 1).otherwise(0)\n",
    "    )\n",
    "    flights_df = flights_df.withColumn(\"label\", col(\"is_delayed\").cast(DoubleType()))\n",
    "    available_cols = [col_name.lower() for col_name in\n",
    "                     [\"FL_DATE\", \"AIRLINE_CODE\", \"FL_NUMBER\", \"ORIGIN\", \"ORIGIN_CITY\",\n",
    "                      \"DEST\", \"DEST_CITY\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\",\n",
    "                      \"CRS_ELAPSED_TIME\", \"DISTANCE\"]\n",
    "                     if col_name.lower() in flights_df.columns]\n",
    "    flights_df = flights_df.select([\"label\"] + available_cols)\n",
    "    for col_name in available_cols:\n",
    "        flights_df = flights_df.filter(col(col_name).isNotNull())\n",
    "\n",
    "    return flights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc4940e-d0d7-4074-8928-a1de2190710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data: 782371 delayed flights, 1616753 on-time flights\n",
      "Class imbalance ratio: 2.07\n",
      "Undersampling on-time flights with fraction: 0.4839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training data: 782371 delayed flights, 783450 on-time flights\n",
      "New class ratio: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training dataset size: 1565821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size (original distribution): 600862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced train and test datasets to HDFS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, fl_date='2021-12-21', airline_code='DL', fl_number=660, origin='ATL', origin_city='Atlanta, GA', dest='LAS', dest_city='Las Vegas, NV', crs_dep_time=959, crs_arr_time=1130, crs_elapsed_time=271.0, distance=1747.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"team21_projectdb_v4.flights_optimized\"\n",
    "flights_df = prepare_data(data_path)\n",
    "\n",
    "# Optional: Take a smaller sample for development\n",
    "sample_fraction = 0.001\n",
    "use_sample = False\n",
    "\n",
    "if use_sample:\n",
    "    print(f\"Using {sample_fraction*100}% sample of data for model development\")\n",
    "    flights_df = flights_df.sample(fraction=sample_fraction, seed=42)\n",
    "\n",
    "train_data, test_data = flights_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "positive_count = train_data.filter(train_data.label == 1.0).count()\n",
    "negative_count = train_data.filter(train_data.label == 0.0).count()\n",
    "\n",
    "print(f\"Original training data: {positive_count} delayed flights, {negative_count} on-time flights\")\n",
    "print(f\"Class imbalance ratio: {negative_count / positive_count:.2f}\")\n",
    "\n",
    "negative_data = train_data.filter(train_data.label == 0.0)\n",
    "positive_data = train_data.filter(train_data.label == 1.0)\n",
    "sampling_fraction = float(positive_count) / float(negative_count)\n",
    "\n",
    "print(f\"Undersampling on-time flights with fraction: {sampling_fraction:.4f}\")\n",
    "\n",
    "undersampled_negative = negative_data.sample(fraction=sampling_fraction, seed=42)\n",
    "balanced_train_data = positive_data.union(undersampled_negative)\n",
    "shuffled_df = balanced_train_data.orderBy(rand())\n",
    "\n",
    "balanced_pos_count = balanced_train_data.filter(balanced_train_data.label == 1.0).count()\n",
    "balanced_neg_count = balanced_train_data.filter(balanced_train_data.label == 0.0).count()\n",
    "\n",
    "print(f\"Balanced training data: {balanced_pos_count} delayed flights, {balanced_neg_count} on-time flights\")\n",
    "print(f\"New class ratio: {balanced_neg_count / balanced_pos_count:.2f}\")\n",
    "\n",
    "balanced_train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"Balanced training dataset size: {balanced_train_data.count()}\")\n",
    "print(f\"Test dataset size (original distribution): {test_data.count()}\")\n",
    "\n",
    "balanced_train_data.write.mode(\"overwrite\").format(\"json\").save(\"/user/team21/project/data/train\")\n",
    "test_data.write.mode(\"overwrite\").format(\"json\").save(\"/user/team21/project/data/test\")\n",
    "print(\"Saved balanced train and test datasets to HDFS\")\n",
    "\n",
    "train_data = balanced_train_data\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4459579a-4d0b-4044-99c0-e0011d9636f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>(108 + 2) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions after repartition: 4\n"
     ]
    }
   ],
   "source": [
    "flights_df = flights_df.repartition(4)\n",
    "print(\"Partitions after repartition:\", flights_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cae6f2-6258-4c40-9303-24db79b98c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using categorical columns: ['airline_code', 'origin', 'dest']\n",
      "Using numeric columns: ['fl_number', 'crs_elapsed_time', 'distance']\n",
      "Applying feature preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------+-----------------+-----------------+------------------------+----------------+-------------------+-------------------+\n",
      "|label|airline_code_encoded| origin_encoded|     dest_encoded|fl_number_imputed|crs_elapsed_time_imputed|distance_imputed|       dep_time_sin|       dep_time_cos|\n",
      "+-----+--------------------+---------------+-----------------+-----------------+------------------------+----------------+-------------------+-------------------+\n",
      "|  1.0|      (19,[9],[1.0])|(381,[0],[1.0])|(381,[106],[1.0])|             3302|                   112.0|           646.0| 0.2546019482055273|-0.9670459389139432|\n",
      "|  1.0|      (19,[9],[1.0])|(381,[0],[1.0])|(381,[182],[1.0])|             3442|                   113.0|           500.0| 0.6946583704589975| -0.719339800338651|\n",
      "|  1.0|      (19,[9],[1.0])|(381,[0],[1.0])| (381,[99],[1.0])|             5159|                   134.0|           780.0|-0.6360782202777636|-0.7716245833877202|\n",
      "|  1.0|      (19,[1],[1.0])|(381,[1],[1.0])| (381,[50],[1.0])|               12|                   131.0|           918.0|-0.8241261886220151|-0.5664062369248336|\n",
      "|  1.0|      (19,[1],[1.0])|(381,[2],[1.0])|  (381,[4],[1.0])|              616|                   202.0|          1337.0| 0.9930684569549263| 0.1175373974578377|\n",
      "+-----+--------------------+---------------+-----------------+-----------------+------------------------+----------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Feature columns for vector assembler: ['airline_code_encoded', 'origin_encoded', 'dest_encoded', 'fl_number_imputed', 'crs_elapsed_time_imputed', 'distance_imputed', 'dep_time_sin', 'dep_time_cos', 'arr_time_sin', 'arr_time_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 20:29:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final feature vectors with weights:\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(792,[9,19,506,78...|\n",
      "|  1.0|(792,[9,19,582,78...|\n",
      "|  1.0|(792,[9,19,499,78...|\n",
      "|  1.0|(792,[1,20,450,78...|\n",
      "|  1.0|(792,[1,21,404,78...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [col.lower() for col in [\"AIRLINE_CODE\", \"ORIGIN\", \"DEST\"]]\n",
    "print(f\"Using categorical columns: {categorical_cols}\")\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", handleInvalid=\"keep\")\n",
    "           for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_indexed\", outputCol=f\"{c}_encoded\", handleInvalid=\"keep\")\n",
    "           for c in categorical_cols]\n",
    "\n",
    "time_transformer = TimeFeatureTransformer()\n",
    "numeric_cols = [col.lower() for col in [\"FL_NUMBER\", \"CRS_ELAPSED_TIME\", \"DISTANCE\"]]\n",
    "\n",
    "print(f\"Using numeric columns: {numeric_cols}\")\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{col}_imputed\" for col in numeric_cols],\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "numeric_cols_imputed = [f\"{col}_imputed\" for col in numeric_cols]\n",
    "preprocessing_stages = indexers + encoders + [time_transformer, imputer]\n",
    "preprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "\n",
    "print(\"Applying feature preprocessing...\")\n",
    "preprocessing_model = preprocessing_pipeline.fit(train_data)\n",
    "train_preprocessed = preprocessing_model.transform(train_data)\n",
    "test_preprocessed = preprocessing_model.transform(test_data)\n",
    "\n",
    "print(\"Preprocessed data sample:\")\n",
    "train_preprocessed.select(\"label\", *([f\"{c}_encoded\" for c in categorical_cols] +\n",
    "                                    numeric_cols_imputed +\n",
    "                                    [\"dep_time_sin\", \"dep_time_cos\"])).show(5)\n",
    "\n",
    "for column in numeric_cols_imputed:\n",
    "    train_preprocessed = train_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "    test_preprocessed = test_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "\n",
    "for column in [\"dep_time_sin\", \"dep_time_cos\", \"arr_time_sin\", \"arr_time_cos\"]:\n",
    "    train_preprocessed = train_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "    test_preprocessed = test_preprocessed.withColumn(\n",
    "        column,\n",
    "        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "    )\n",
    "\n",
    "if \"fl_date\" in train_data.columns:\n",
    "    for column in [\"month_sin\", \"month_cos\", \"dayofweek_sin\", \"dayofweek_cos\"]:\n",
    "        if column in train_preprocessed.columns:\n",
    "            train_preprocessed = train_preprocessed.withColumn(\n",
    "                column,\n",
    "                when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "            )\n",
    "            test_preprocessed = test_preprocessed.withColumn(\n",
    "                column,\n",
    "                when(col(column).isNull() | isnan(column), 0).otherwise(col(column))\n",
    "            )\n",
    "\n",
    "feature_cols = []\n",
    "if categorical_cols:\n",
    "    feature_cols.extend([f\"{c}_encoded\" for c in categorical_cols])\n",
    "feature_cols.extend(numeric_cols_imputed)\n",
    "feature_cols.extend([\"dep_time_sin\", \"dep_time_cos\", \"arr_time_sin\", \"arr_time_cos\"])\n",
    "if \"fl_date\" in train_data.columns:\n",
    "    feature_cols.extend([\"month_sin\", \"month_cos\", \"dayofweek_sin\", \"dayofweek_cos\"])\n",
    "\n",
    "print(f\"\\nFeature columns for vector assembler: {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withMean=False,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[assembler, scaler])\n",
    "feature_model = feature_pipeline.fit(train_preprocessed)\n",
    "train_vectorized = feature_model.transform(train_preprocessed)\n",
    "test_vectorized = feature_model.transform(test_preprocessed)\n",
    "\n",
    "train_vectorized = train_vectorized.withColumn(\"features\", col(\"features_scaled\"))\n",
    "test_vectorized = test_vectorized.withColumn(\"features\", col(\"features_scaled\"))\n",
    "\n",
    "train_vectorized = train_vectorized.na.drop(subset=[\"features\"])\n",
    "test_vectorized = test_vectorized.na.drop(subset=[\"features\"])\n",
    "\n",
    "\n",
    "print(\"\\nFinal feature vectors with weights:\")\n",
    "train_vectorized.select(\"label\", \"features\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c87447-ce41-4601-955a-1b6e42b9a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 20:32:29 WARN DAGScheduler: Broadcasting large task binary with size 1204.8 KiB\n",
      "25/05/09 20:32:52 WARN DAGScheduler: Broadcasting large task binary with size 1587.6 KiB\n",
      "25/05/09 20:33:17 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/05/09 20:33:47 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/09 20:34:50 WARN DAGScheduler: Broadcasting large task binary with size 1594.3 KiB\n",
      "25/05/09 20:37:09 WARN DAGScheduler: Broadcasting large task binary with size 1204.8 KiB\n",
      "25/05/09 20:37:34 WARN DAGScheduler: Broadcasting large task binary with size 1587.6 KiB\n",
      "25/05/09 20:38:01 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/05/09 20:38:32 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/09 20:39:13 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/09 20:40:01 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/05/09 20:40:52 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "25/05/09 20:41:45 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "25/05/09 20:42:49 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "25/05/09 20:43:45 WARN DAGScheduler: Broadcasting large task binary with size 1038.0 KiB\n",
      "25/05/09 20:44:02 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/05/09 20:46:26 WARN DAGScheduler: Broadcasting large task binary with size 1017.7 KiB\n",
      "25/05/09 20:46:56 WARN DAGScheduler: Broadcasting large task binary with size 1361.7 KiB\n",
      "25/05/09 20:47:31 WARN DAGScheduler: Broadcasting large task binary with size 1874.5 KiB\n",
      "25/05/09 20:48:12 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/09 20:49:00 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/09 20:50:04 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "25/05/09 20:51:28 WARN BlockManagerMaster: Failed to remove RDD 441 - Block rdd_441_84 does not exist\n",
      "org.apache.spark.SparkException: Block rdd_441_84 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2093)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2057)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1993)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/09 20:51:28 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/05/09 20:53:49 WARN DAGScheduler: Broadcasting large task binary with size 1017.7 KiB\n",
      "25/05/09 20:54:19 WARN DAGScheduler: Broadcasting large task binary with size 1361.7 KiB\n",
      "25/05/09 20:54:55 WARN DAGScheduler: Broadcasting large task binary with size 1874.5 KiB\n",
      "25/05/09 20:55:36 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/09 20:56:27 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/05/09 20:57:27 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "[Stage 195:====================================>                (151 + 3) / 220]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/multiprocessing/pool.py:856\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      8\u001b[39m rf_evaluator = BinaryClassificationEvaluator(labelCol=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, metricName=\u001b[33m\"\u001b[39m\u001b[33mareaUnderROC\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m rf_cv = CrossValidator(estimator=rf,\n\u001b[32m     10\u001b[39m                       estimatorParamMaps=rf_param_grid,\n\u001b[32m     11\u001b[39m                       evaluator=rf_evaluator,\n\u001b[32m     12\u001b[39m                       numFolds=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m rf_model = \u001b[43mrf_cv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m best_rf_model = rf_model.bestModel\n\u001b[32m     17\u001b[39m best_rf_model.write().overwrite().save(\u001b[33m\"\u001b[39m\u001b[33mproject/models/model1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[39m, in \u001b[36mCrossValidator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    841\u001b[39m train = datasets[i][\u001b[32m0\u001b[39m].cache()\n\u001b[32m    843\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    844\u001b[39m     inheritable_thread_target,\n\u001b[32m    845\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    846\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/multiprocessing/pool.py:861\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    863\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 195:====================================>                (153 + 2) / 220]\r"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "rf_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "rf_cv = CrossValidator(estimator=rf,\n",
    "                      estimatorParamMaps=rf_param_grid,\n",
    "                      evaluator=rf_evaluator,\n",
    "                      numFolds=3)\n",
    "\n",
    "rf_model = rf_cv.fit(train_vectorized)\n",
    "best_rf_model = rf_model.bestModel\n",
    "\n",
    "best_rf_model.write().overwrite().save(\"project/models/model1\")\n",
    "rf_predictions = best_rf_model.transform(test_vectorized)\n",
    "\n",
    "rf_roc = rf_evaluator.evaluate(rf_predictions)\n",
    "rf_evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "rf_precision = rf_evaluator_precision.evaluate(rf_predictions)\n",
    "rf_evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "rf_recall = rf_evaluator_recall.evaluate(rf_predictions)\n",
    "rf_evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "rf_f1 = rf_evaluator_f1.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random forest - AUC: {rf_roc}, Precision: {rf_precision}, Recall: {rf_recall}, F1: {rf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73bc9eb-2ea5-4fbb-8e01-5a5919d5052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best maxDepth: {best_rf_model.getMaxDepth()}\")\n",
    "print(f\"Best numTrees: {best_rf_model.getNumTrees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d3846-1f87-4f03-a591-e7c537615693",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\"hdfs dfs -get project/models/model1 \\\n",
    "~/flight-delay-prediction/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f7f42-aad2-409a-a12b-f6cf93c4c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468ecad-e447-4409-be4f-ee6a2a442072",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions.select(\"label\", \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .csv(\"/user/team21/project/output/model1_predictions\", header=True, mode=\"overwrite\")\n",
    "\n",
    "run(\"hdfs dfs -mv /user/team21/project/output/model1_predictions/*.csv /user/team21/project/output/model1_predictions/model1_predictions.csv\")\n",
    "run(\"hdfs dfs -cat /user/team21/project/output/model1_predictions/model1_predictions.csv > ~/flight-delay-prediction/output/model1_predictions.csv\")\n",
    "run(\"hdfs dfs -get /user/team21/project/models/model1 ~/project/big-data-pipeline-project/models/.\")\n",
    "\n",
    "# After making predictions, calculate per-class metrics\n",
    "rf_tp = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_fp = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_tn = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "rf_fn = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nDetailed Classification Metrics:\")\n",
    "print(f\"  True Positives: {rf_tp}\")\n",
    "print(f\"  False Positives: {rf_fp}\")\n",
    "print(f\"  True Negatives: {rf_tn}\")\n",
    "print(f\"  False Negatives: {rf_fn}\")\n",
    "\n",
    "# For potential threshold adjustment\n",
    "if rf_tn == 0 and rf_fp > 0:\n",
    "    print(\"\\nWARNING: Model is predicting all examples as positive.\")\n",
    "    print(\"Consider adjusting threshold or balancing training data differently.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd155bab-9a49-4615-9078-e05ea171ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probability(v, index):\n",
    "    try:\n",
    "        return float(v[index])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "prob_udf = udf(lambda v: extract_probability(v, 1), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f3a0a-bedc-49f0-ae8e-fbac4f91799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "print(\"Training Decision Tree model with hyperparameter tuning...\")\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [7, 10]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "dt_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "dt_cv = CrossValidator(\n",
    "    estimator=decision_tree,\n",
    "    estimatorParamMaps=dt_param_grid,\n",
    "    evaluator=dt_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "print(\"Performing cross-validation with grid search...\")\n",
    "dt_model = dt_cv.fit(train_vectorized)\n",
    "\n",
    "best_dt_model = dt_model.bestModel\n",
    "print(f\"Best Decision Tree parameters: {best_dt_model.extractParamMap()}\")\n",
    "print(f\"Best maxDepth: {best_dt_model.getMaxDepth()}\")\n",
    "print(f\"Best maxBins: {best_dt_model.getMaxBins()}\")\n",
    "\n",
    "best_dt_model.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "dt_predictions = best_dt_model.transform(test_vectorized)\n",
    "dt_predictions = dt_predictions.withColumn(\n",
    "    \"probability_for_delay\",\n",
    "    prob_udf(col(\"probability\"))\n",
    ")\n",
    "\n",
    "dt_roc = dt_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "dt_evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "dt_precision = dt_evaluator_precision.evaluate(dt_predictions)\n",
    "\n",
    "dt_evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "dt_recall = dt_evaluator_recall.evaluate(dt_predictions)\n",
    "\n",
    "dt_evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "dt_f1 = dt_evaluator_f1.evaluate(dt_predictions)\n",
    "\n",
    "print(\"\\nDecision Tree Model Evaluation:\")\n",
    "print(f\"  AUC: {dt_roc:.4f}\")\n",
    "print(f\"  Precision: {dt_precision:.4f}\")\n",
    "print(f\"  Recall: {dt_recall:.4f}\")\n",
    "print(f\"  F1 Score: {dt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b868c3f-ee33-4a10-a339-e7ab3ac9e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6eea0e-8455-4b8c-89e3-79ac009a8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConfusion Matrix:\")\n",
    "dt_predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "# Detailed metrics\n",
    "tp = dt_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "fp = dt_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "tn = dt_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "fn = dt_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "\n",
    "print(\"\\nDetailed Classification Metrics:\")\n",
    "print(f\"  True Positives: {tp}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  True Negatives: {tn}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "\n",
    "# Save predictions\n",
    "dt_predictions.select(\"label\", \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/user/team21/project/output/model2_predictions\", mode=\"overwrite\")\n",
    "\n",
    "# Move and rename output file\n",
    "run(\"hdfs dfs -mv /user/team21/project/output/model2_predictions/*.csv /user/team21/project/output/model3_predictions/model2_predictions.csv\")\n",
    "run(\"hdfs dfs -cat /user/team21/project/output/model2_predictions/model3_predictions.csv > ~/flight-delay-prediction/output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345274d-c1d1-40b5-bb28-9f9c30823672",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tp = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_fp = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "rf_tn = rf_predictions.filter((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "rf_fn = rf_predictions.filter((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "\n",
    "dt_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "rf_accuracy = (rf_tp + rf_tn) / (rf_tp + rf_tn + rf_fp + rf_fn)\n",
    "\n",
    "comparison_data = {\n",
    "    'model': ['RandomForest', 'DecisionTree'],\n",
    "    'auc': [rf_roc, dt_roc],\n",
    "    'precision': [rf_precision, dt_precision],\n",
    "    'recall': [rf_recall, dt_recall],\n",
    "    'f1': [rf_f1, dt_f1],\n",
    "    'accuracy': [rf_accuracy, dt_accuracy],\n",
    "    'true_positives': [rf_tp, tp],\n",
    "    'false_positives': [rf_fp, fp],\n",
    "    'true_negatives': [rf_tn, tn],\n",
    "    'false_negatives': [rf_fn, fn]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df[['model', 'auc', 'precision', 'recall', 'f1', 'accuracy']])\n",
    "comparison_df.to_csv(\"~/flight-delay-prediction/output/evaluation.csv\", index=False)\n",
    "\n",
    "spark.createDataFrame(comparison_df) \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/user/team21/project/output/evaluation\", mode=\"overwrite\")\n",
    "\n",
    "run(\"hdfs dfs -get /user/team21/project/models/model2 models/\")\n",
    "\n",
    "print(\"\\nAll models and evaluation results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036eb6c-8f21-4acc-9f60-7256a5afc8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76763e-509b-4282-8e03-a9a4738475f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d18fd5-41af-40c9-bcea-8601d924e10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
