# -*- coding: utf-8 -*-
"""ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AsTDqHkTQQKn9PlGPlCjtj4D7K-pMd-U
"""

import os
import math
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, sin, cos, lit, udf, isnan, count, isnull, expr, rand
from pyspark.sql.types import DoubleType, IntegerType, StringType, BooleanType
from pyspark.ml import Pipeline, Transformer
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer
from pyspark.ml.classification import GBTClassifier, RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
from pyspark.ml.linalg import VectorUDT, Vectors
import time

def run(command):
    """Function for executing terminal commands"""
    return os.popen(command).read()

# Add here your team number teamx
team = 21

# location of your Hive database in HDFS
warehouse = "/user/team21/project/hive/warehouse"

spark = SparkSession.builder\
        .appName("{} - spark ML".format(team))\
        .master("yarn")\
        .config("hive.metastore.uris", "thrift://hadoop-02.uni.innopolis.ru:9883")\
        .config("spark.sql.warehouse.dir", warehouse)\
        .config("spark.sql.avro.compression.codec", "snappy")\
        .enableHiveSupport()\
        .getOrCreate()

spark.conf.set("spark.sql.repl.eagerEval.enabled", True)
sc = spark.sparkContext
spark

class TimeFeatureTransformer(Transformer, DefaultParamsReadable, DefaultParamsWritable):
    """
    Transformer to convert time and date features into cyclical features
    Handles NULL values and ensures robustness
    """
    def _transform(self, dataset):
        def parse_time(time_val):
            """Converts time from HHMM format to hours and hour fractions"""
            if time_val is None or time_val < 0:
                return None

            try:
                time_str = str(int(time_val))
                if len(time_str) <= 2:  # Only minutes
                    return float(time_str) / 60.0

                # Hours and minutes
                if len(time_str) == 3:
                    time_str = "0" + time_str
                hours = int(time_str[0:2])
                minutes = int(time_str[2:4])

                return float(hours) + float(minutes) / 60.0
            except:
                return None

        # Function to extract month from date in yyyymmdd format
        def get_month_from_date(date_val):
            if date_val is None or date_val == "":
                return None

            try:
                date_str = str(date_val).strip().split('-')
                return int(date_str[1])  # month
            except:
                return None

        # determine day of week (1=Monday, 7=Sunday)
        def get_day_of_week(date_val):
            """Calculates day of week from date in yyyymmdd format"""
            if date_val is None or date_val == "":
                return None

            try:
                date_str = str(date_val).strip().split('-')
                year = int(int(date_str[0]))
                month = int(date_str[1])
                day = int(date_str[2])
                if year < 1900 or year > 2100 or month < 1 or month > 12 or day < 1 or day > 31:
                    return None
                if month < 3:
                    month += 12
                    year -= 1

                k = year % 100
                j = year // 100

                h = (day + ((13 * (month + 1)) // 5) + k + (k // 4) + (j // 4) - (2 * j)) % 7
                day_of_week = ((h + 5) % 7) + 1

                return day_of_week
            except:
                return None

        parse_time_udf = udf(parse_time, DoubleType())
        get_month_udf = udf(get_month_from_date, IntegerType())
        get_day_of_week_udf = udf(get_day_of_week, IntegerType())
        dataset = dataset.withColumn("dep_time_decimal",
                              when(col("CRS_DEP_TIME").isNotNull(),
                                   parse_time_udf(col("CRS_DEP_TIME"))).otherwise(0.0))
        dataset = dataset.withColumn("arr_time_decimal",
                              when(col("CRS_ARR_TIME").isNotNull(),
                                   parse_time_udf(col("CRS_ARR_TIME"))).otherwise(0.0))
        dataset = dataset.withColumn("dep_time_sin",
                              sin(col("dep_time_decimal") * 2 * math.pi / 24))
        dataset = dataset.withColumn("dep_time_cos",
                              cos(col("dep_time_decimal") * 2 * math.pi / 24))
        dataset = dataset.withColumn("arr_time_sin",
                              sin(col("arr_time_decimal") * 2 * math.pi / 24))
        dataset = dataset.withColumn("arr_time_cos",
                              cos(col("arr_time_decimal") * 2 * math.pi / 24))
        dataset = dataset.withColumn("month",
                          when(col("fl_date").isNotNull(),
                                get_month_udf(col("fl_date"))).otherwise(6))
        dataset = dataset.withColumn("dayofweek",
                              when(col("fl_date").isNotNull(),
                                    get_day_of_week_udf(col("fl_date"))).otherwise(4))

        dataset = dataset.withColumn("month_sin", sin(col("month") * 2 * math.pi / 12))
        dataset = dataset.withColumn("month_cos", cos(col("month") * 2 * math.pi / 12))
        dataset = dataset.withColumn("dayofweek_sin", sin(col("dayofweek") * 2 * math.pi / 7))
        dataset = dataset.withColumn("dayofweek_cos", cos(col("dayofweek") * 2 * math.pi / 7))

        return dataset

def prepare_data(full_table_name):
    flights_df = spark.read.table(full_table_name)
    flights_df = flights_df.withColumn("fl_date", col("fl_date").cast("string"))
    flights_df = flights_df.withColumn(
        "is_delayed",
        when((col("ARR_DELAY").isNotNull()) & (col("ARR_DELAY") > 0), 1).otherwise(0)
    )
    flights_df = flights_df.withColumn("label", col("is_delayed").cast(DoubleType()))
    available_cols = [col_name.lower() for col_name in
                     ["FL_DATE", "AIRLINE_CODE", "FL_NUMBER", "ORIGIN", "ORIGIN_CITY",
                      "DEST", "DEST_CITY", "CRS_DEP_TIME", "CRS_ARR_TIME",
                      "CRS_ELAPSED_TIME", "DISTANCE"]
                     if col_name.lower() in flights_df.columns]
    flights_df = flights_df.select(["label"] + available_cols)
    for col_name in available_cols:
        flights_df = flights_df.filter(col(col_name).isNotNull())

    return flights_df

data_path = "team21_projectdb_v4.flights_optimized"
flights_df = prepare_data(data_path)

# Optional: Take a smaller sample for development
sample_fraction = 0.001
use_sample = False

if use_sample:
    print(f"Using {sample_fraction*100}% sample of data for model development")
    flights_df = flights_df.sample(fraction=sample_fraction, seed=42)

train_data, test_data = flights_df.randomSplit([0.8, 0.2], seed=42)

positive_count = train_data.filter(train_data.label == 1.0).count()
negative_count = train_data.filter(train_data.label == 0.0).count()

print(f"Original training data: {positive_count} delayed flights, {negative_count} on-time flights")
print(f"Class imbalance ratio: {negative_count / positive_count:.2f}")

negative_data = train_data.filter(train_data.label == 0.0)
positive_data = train_data.filter(train_data.label == 1.0)
sampling_fraction = float(positive_count) / float(negative_count)

print(f"Undersampling on-time flights with fraction: {sampling_fraction:.4f}")

undersampled_negative = negative_data.sample(fraction=sampling_fraction, seed=42)
balanced_train_data = positive_data.union(undersampled_negative)
shuffled_df = balanced_train_data.orderBy(rand())

balanced_pos_count = balanced_train_data.filter(balanced_train_data.label == 1.0).count()
balanced_neg_count = balanced_train_data.filter(balanced_train_data.label == 0.0).count()

print(f"Balanced training data: {balanced_pos_count} delayed flights, {balanced_neg_count} on-time flights")
print(f"New class ratio: {balanced_neg_count / balanced_pos_count:.2f}")

balanced_train_data.cache()
test_data.cache()

print(f"Balanced training dataset size: {balanced_train_data.count()}")
print(f"Test dataset size (original distribution): {test_data.count()}")

balanced_train_data.write.mode("overwrite").format("json").save("/user/team21/project/data/train")
test_data.write.mode("overwrite").format("json").save("/user/team21/project/data/test")
print("Saved balanced train and test datasets to HDFS")

train_data = balanced_train_data
flights_df.head()

flights_df = flights_df.repartition(4)
print("Partitions after repartition:", flights_df.rdd.getNumPartitions())

categorical_cols = [col.lower() for col in ["AIRLINE_CODE", "ORIGIN", "DEST"]]
print(f"Using categorical columns: {categorical_cols}")

indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_indexed", handleInvalid="keep")
           for c in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_indexed", outputCol=f"{c}_encoded", handleInvalid="keep")
           for c in categorical_cols]

time_transformer = TimeFeatureTransformer()
numeric_cols = [col.lower() for col in ["FL_NUMBER", "CRS_ELAPSED_TIME", "DISTANCE"]]

print(f"Using numeric columns: {numeric_cols}")

imputer = Imputer(
    inputCols=numeric_cols,
    outputCols=[f"{col}_imputed" for col in numeric_cols],
    strategy="mean"
)

numeric_cols_imputed = [f"{col}_imputed" for col in numeric_cols]
preprocessing_stages = indexers + encoders + [time_transformer, imputer]
preprocessing_pipeline = Pipeline(stages=preprocessing_stages)

print("Applying feature preprocessing...")
preprocessing_model = preprocessing_pipeline.fit(train_data)
train_preprocessed = preprocessing_model.transform(train_data)
test_preprocessed = preprocessing_model.transform(test_data)

print("Preprocessed data sample:")
train_preprocessed.select("label", *([f"{c}_encoded" for c in categorical_cols] +
                                    numeric_cols_imputed +
                                    ["dep_time_sin", "dep_time_cos"])).show(5)

for column in numeric_cols_imputed:
    train_preprocessed = train_preprocessed.withColumn(
        column,
        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))
    )
    test_preprocessed = test_preprocessed.withColumn(
        column,
        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))
    )

for column in ["dep_time_sin", "dep_time_cos", "arr_time_sin", "arr_time_cos"]:
    train_preprocessed = train_preprocessed.withColumn(
        column,
        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))
    )
    test_preprocessed = test_preprocessed.withColumn(
        column,
        when(col(column).isNull() | isnan(column), 0).otherwise(col(column))
    )

if "fl_date" in train_data.columns:
    for column in ["month_sin", "month_cos", "dayofweek_sin", "dayofweek_cos"]:
        if column in train_preprocessed.columns:
            train_preprocessed = train_preprocessed.withColumn(
                column,
                when(col(column).isNull() | isnan(column), 0).otherwise(col(column))
            )
            test_preprocessed = test_preprocessed.withColumn(
                column,
                when(col(column).isNull() | isnan(column), 0).otherwise(col(column))
            )

feature_cols = []
if categorical_cols:
    feature_cols.extend([f"{c}_encoded" for c in categorical_cols])
feature_cols.extend(numeric_cols_imputed)
feature_cols.extend(["dep_time_sin", "dep_time_cos", "arr_time_sin", "arr_time_cos"])
if "fl_date" in train_data.columns:
    feature_cols.extend(["month_sin", "month_cos", "dayofweek_sin", "dayofweek_cos"])

print(f"\nFeature columns for vector assembler: {feature_cols}")

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features_raw",
    handleInvalid="skip"
)

scaler = StandardScaler(
    inputCol="features_raw",
    outputCol="features_scaled",
    withMean=False,
    withStd=True
)

feature_pipeline = Pipeline(stages=[assembler, scaler])
feature_model = feature_pipeline.fit(train_preprocessed)
train_vectorized = feature_model.transform(train_preprocessed)
test_vectorized = feature_model.transform(test_preprocessed)

train_vectorized = train_vectorized.withColumn("features", col("features_scaled"))
test_vectorized = test_vectorized.withColumn("features", col("features_scaled"))

train_vectorized = train_vectorized.na.drop(subset=["features"])
test_vectorized = test_vectorized.na.drop(subset=["features"])


print("\nFinal feature vectors with weights:")
train_vectorized.select("label", "features").show(5, truncate=True)

rf = RandomForestClassifier(labelCol="label", featuresCol="features")

rf_param_grid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [50, 100]) \
    .addGrid(rf.maxDepth, [10, 15]) \
    .build()

rf_evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
rf_cv = CrossValidator(estimator=rf,
                      estimatorParamMaps=rf_param_grid,
                      evaluator=rf_evaluator,
                      numFolds=3)

rf_model = rf_cv.fit(train_vectorized)
best_rf_model = rf_model.bestModel

best_rf_model.write().overwrite().save("project/models/model1")
rf_predictions = best_rf_model.transform(test_vectorized)

rf_roc = rf_evaluator.evaluate(rf_predictions)
rf_evaluator_precision = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision")
rf_precision = rf_evaluator_precision.evaluate(rf_predictions)
rf_evaluator_recall = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall")
rf_recall = rf_evaluator_recall.evaluate(rf_predictions)
rf_evaluator_f1 = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
rf_f1 = rf_evaluator_f1.evaluate(rf_predictions)

print(f"Random forest - AUC: {rf_roc}, Precision: {rf_precision}, Recall: {rf_recall}, F1: {rf_f1}")

print(f"Best maxDepth: {best_rf_model.getMaxDepth()}")
print(f"Best numTrees: {best_rf_model.getNumTrees}")

run("hdfs dfs -get project/models/model1 \
~/flight-delay-prediction/models/")

time.sleep(60)

rf_predictions.select("label", "prediction") \
    .coalesce(1) \
    .write \
    .csv("/user/team21/project/output/model1_predictions", header=True, mode="overwrite")

run("hdfs dfs -mv /user/team21/project/output/model1_predictions/*.csv /user/team21/project/output/model1_predictions/model1_predictions.csv")
run("hdfs dfs -cat /user/team21/project/output/model1_predictions/model1_predictions.csv > ~/flight-delay-prediction/output/model1_predictions.csv")
run("hdfs dfs -get /user/team21/project/models/model1 ~/project/big-data-pipeline-project/models/.")

# After making predictions, calculate per-class metrics
rf_tp = rf_predictions.filter((col("label") == 1.0) & (col("prediction") == 1.0)).count()
rf_fp = rf_predictions.filter((col("label") == 0.0) & (col("prediction") == 1.0)).count()
rf_tn = rf_predictions.filter((col("label") == 0.0) & (col("prediction") == 0.0)).count()
rf_fn = rf_predictions.filter((col("label") == 1.0) & (col("prediction") == 0.0)).count()

# Print detailed metrics
print("\nDetailed Classification Metrics:")
print(f"  True Positives: {rf_tp}")
print(f"  False Positives: {rf_fp}")
print(f"  True Negatives: {rf_tn}")
print(f"  False Negatives: {rf_fn}")

# For potential threshold adjustment
if rf_tn == 0 and rf_fp > 0:
    print("\nWARNING: Model is predicting all examples as positive.")
    print("Consider adjusting threshold or balancing training data differently.")

def extract_probability(v, index):
    try:
        return float(v[index])
    except:
        return None

prob_udf = udf(lambda v: extract_probability(v, 1), DoubleType())

from pyspark.ml.classification import DecisionTreeClassifier

print("Training Decision Tree model with hyperparameter tuning...")
decision_tree = DecisionTreeClassifier(
    labelCol="label",
    featuresCol="features"
)

dt_param_grid = ParamGridBuilder() \
    .addGrid(decision_tree.maxDepth, [7, 10]) \
    .addGrid(decision_tree.maxBins, [32, 64]) \
    .build()

dt_evaluator = BinaryClassificationEvaluator(
    labelCol="label",
    metricName="areaUnderROC"
)

dt_cv = CrossValidator(
    estimator=decision_tree,
    estimatorParamMaps=dt_param_grid,
    evaluator=dt_evaluator,
    numFolds=3
)

print("Performing cross-validation with grid search...")
dt_model = dt_cv.fit(train_vectorized)

best_dt_model = dt_model.bestModel
print(f"Best Decision Tree parameters: {best_dt_model.extractParamMap()}")
print(f"Best maxDepth: {best_dt_model.getMaxDepth()}")
print(f"Best maxBins: {best_dt_model.getMaxBins()}")

best_dt_model.write().overwrite().save("project/models/model2")

dt_predictions = best_dt_model.transform(test_vectorized)
dt_predictions = dt_predictions.withColumn(
    "probability_for_delay",
    prob_udf(col("probability"))
)

dt_roc = dt_evaluator.evaluate(dt_predictions)

dt_evaluator_precision = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="weightedPrecision"
)
dt_precision = dt_evaluator_precision.evaluate(dt_predictions)

dt_evaluator_recall = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="weightedRecall"
)
dt_recall = dt_evaluator_recall.evaluate(dt_predictions)

dt_evaluator_f1 = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="f1"
)
dt_f1 = dt_evaluator_f1.evaluate(dt_predictions)

print("\nDecision Tree Model Evaluation:")
print(f"  AUC: {dt_roc:.4f}")
print(f"  Precision: {dt_precision:.4f}")
print(f"  Recall: {dt_recall:.4f}")
print(f"  F1 Score: {dt_f1:.4f}")

time.sleep(60)

print("\nConfusion Matrix:")
dt_predictions.groupBy("label", "prediction").count().show()

# Detailed metrics
tp = dt_predictions.filter((col("label") == 1.0) & (col("prediction") == 1.0)).count()
fp = dt_predictions.filter((col("label") == 0.0) & (col("prediction") == 1.0)).count()
tn = dt_predictions.filter((col("label") == 0.0) & (col("prediction") == 0.0)).count()
fn = dt_predictions.filter((col("label") == 1.0) & (col("prediction") == 0.0)).count()

print("\nDetailed Classification Metrics:")
print(f"  True Positives: {tp}")
print(f"  False Positives: {fp}")
print(f"  True Negatives: {tn}")
print(f"  False Negatives: {fn}")

# Save predictions
dt_predictions.select("label", "prediction") \
    .coalesce(1) \
    .write \
    .option("header", "true") \
    .csv("/user/team21/project/output/model2_predictions", mode="overwrite")

# Move and rename output file
run("hdfs dfs -mv /user/team21/project/output/model2_predictions/*.csv /user/team21/project/output/model3_predictions/model2_predictions.csv")
run("hdfs dfs -cat /user/team21/project/output/model2_predictions/model3_predictions.csv > ~/flight-delay-prediction/output/model2_predictions.csv")

rf_tp = rf_predictions.filter((col("label") == 1.0) & (col("prediction") == 1.0)).count()
rf_fp = rf_predictions.filter((col("label") == 0.0) & (col("prediction") == 1.0)).count()
rf_tn = rf_predictions.filter((col("label") == 0.0) & (col("prediction") == 0.0)).count()
rf_fn = rf_predictions.filter((col("label") == 1.0) & (col("prediction") == 0.0)).count()

dt_accuracy = (tp + tn) / (tp + tn + fp + fn)
rf_accuracy = (rf_tp + rf_tn) / (rf_tp + rf_tn + rf_fp + rf_fn)

comparison_data = {
    'model': ['RandomForest', 'DecisionTree'],
    'auc': [rf_roc, dt_roc],
    'precision': [rf_precision, dt_precision],
    'recall': [rf_recall, dt_recall],
    'f1': [rf_f1, dt_f1],
    'accuracy': [rf_accuracy, dt_accuracy],
    'true_positives': [rf_tp, tp],
    'false_positives': [rf_fp, fp],
    'true_negatives': [rf_tn, tn],
    'false_negatives': [rf_fn, fn]
}

comparison_df = pd.DataFrame(comparison_data)
print("\nModel Comparison:")
print(comparison_df[['model', 'auc', 'precision', 'recall', 'f1', 'accuracy']])
comparison_df.to_csv("~/flight-delay-prediction/output/evaluation.csv", index=False)

spark.createDataFrame(comparison_df) \
    .coalesce(1) \
    .write \
    .option("header", "true") \
    .csv("/user/team21/project/output/evaluation", mode="overwrite")

run("hdfs dfs -get /user/team21/project/models/model2 models/")

print("\nAll models and evaluation results saved successfully!")

